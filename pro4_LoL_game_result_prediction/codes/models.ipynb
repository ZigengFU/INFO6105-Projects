{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0     2     3     4     5     6     7     8     9     10  ...     42  \\\n",
      "0    1  4.32  2.88  1.28     4  2.56     2  2.24  5.12  5.36  ...  0.683   \n",
      "1    0  2.64  2.32  1.92  3.92  2.16   1.6  2.08     4  2.88  ...  0.714   \n",
      "2    0  2.64  2.32  1.92  3.92  2.16   1.6  2.08     4  2.88  ...  0.714   \n",
      "3    1   2.8   1.6   1.2  2.64   1.6  0.96  2.16  3.36  2.32  ...  0.664   \n",
      "4    0   2.8   1.6   1.2  2.64   1.6  0.96  2.16  3.36  2.32  ...  0.664   \n",
      "5    1  2.64  2.48  1.68  3.04  3.52   1.6  1.36  4.24  2.88  ...  0.703   \n",
      "6    1  2.64  2.48  1.68  3.04  3.52   1.6  1.36  4.24  2.88  ...  0.703   \n",
      "7    1  2.88  2.16  1.52  3.12  1.92  1.04  2.72  5.44  2.16  ...  0.682   \n",
      "8    1  2.88  2.16  1.52  3.12  1.92  1.04  2.72  5.44  2.16  ...  0.682   \n",
      "9    1  3.76  2.16  1.12  3.28     2   1.6  2.08   3.6  2.72  ...  0.655   \n",
      "10   0  3.76  2.16  1.12  3.28     2   1.6  2.08   3.6  2.72  ...  0.655   \n",
      "11   1   3.6  1.68  0.96  2.64  2.16  1.92  1.92  3.28  2.16  ...  0.628   \n",
      "12   1   3.6  1.68  0.96  2.64  2.16  1.92  1.92  3.28  2.16  ...  0.628   \n",
      "13   1  5.28   3.2   1.2  4.48  3.76   1.6  1.52  5.36  7.04  ...   0.73   \n",
      "14   1  5.28   3.2   1.2  4.48  3.76   1.6  1.52  5.36  7.04  ...   0.73   \n",
      "15   1  4.88  2.56  1.12  4.24  2.56  2.16  2.08  4.56   5.2  ...   0.69   \n",
      "16   1  4.88  2.56  1.12  4.24  2.56  2.16  2.08  4.56   5.2  ...   0.69   \n",
      "17   1   4.8   3.1   1.7   4.9   2.9   1.7   2.7   6.4   4.4  ...  0.699   \n",
      "18   1   4.8   3.1   1.7   4.9   2.9   1.7   2.7   6.4   4.4  ...  0.699   \n",
      "19   1   4.2   3.4   1.6   3.3   2.7   1.1   2.1   4.7   2.4  ...  0.771   \n",
      "20   0   4.2   3.4   1.6   3.3   2.7   1.1   2.1   4.7   2.4  ...  0.771   \n",
      "21   0   7.3   3.5   1.1   4.4   7.8   3.1   1.3     7   8.4  ...  0.585   \n",
      "22   1   7.3   3.5   1.1   4.4   7.8   3.1   1.3     7   8.4  ...  0.585   \n",
      "23   1   4.1   2.9   1.7   3.8   2.4   1.3   2.7   5.2   2.9  ...  0.716   \n",
      "24   1   4.1   2.9   1.7   3.8   2.4   1.3   2.7   5.2   2.9  ...  0.716   \n",
      "25   1   1.9     2   2.1   2.1   3.1   1.2   1.2   2.5   1.9  ...  0.592   \n",
      "26   0   1.9     2   2.1   2.1   3.1   1.2   1.2   2.5   1.9  ...  0.592   \n",
      "27   0   2.8   2.8   2.4   3.9   2.7   2.2   2.8   5.2     3  ...  0.664   \n",
      "28   1   2.8   2.8   2.4   3.9   2.7   2.2   2.8   5.2     3  ...  0.664   \n",
      "29   1   5.2   3.7   1.8   5.6   3.8   2.6   2.2   5.8   4.8  ...  0.717   \n",
      "..  ..   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...    ...   \n",
      "914  1   3.7   3.7   2.3   4.7     3   1.9   3.1   7.5   3.4  ...  0.689   \n",
      "915  0   3.7   3.7   2.3   4.7     3   1.9   3.1   7.5   3.4  ...  0.689   \n",
      "916  0   4.1   3.9     2   4.3   3.2   2.3   2.6     6   2.7  ...  0.694   \n",
      "917  0   2.7   2.4   2.5   4.4   2.9   2.2   3.1   6.7   2.6  ...  0.595   \n",
      "918  1   4.7   4.3     2   5.3   3.3   3.2   3.1   7.1   4.3  ...  0.689   \n",
      "919  0   3.9   3.4     2   4.4   2.8     1   2.5   6.1   2.1  ...  0.724   \n",
      "920  1   4.8   4.1   1.8   4.7   3.1   3.4   2.9   5.7   3.5  ...  0.709   \n",
      "921  1     3   3.3   2.7   4.6   3.2     2   2.7   6.7   3.4  ...  0.626   \n",
      "922  0   3.7   3.4   1.9   3.6     3   1.8   2.5   5.8   2.6  ...  0.685   \n",
      "923  0   6.8   3.1   1.2   5.3     4   1.8   2.2   6.9   7.1  ...  0.662   \n",
      "924  0   3.8   3.8   2.3     5     3     2     3   6.9   3.6  ...  0.702   \n",
      "925  0   3.4   3.3   2.2   4.3   2.7   1.4   2.7   5.8   2.5  ...  0.748   \n",
      "926  1  4.08   3.2  1.28  3.36  4.56  2.32  1.28   4.8  4.24  ...  0.615   \n",
      "927  1  4.08   3.2  1.28  3.36  4.56  2.32  1.28   4.8  4.24  ...  0.615   \n",
      "928  1   5.2  3.84  1.36  5.04  3.68  2.32  1.92  6.48  3.68  ...  0.633   \n",
      "929  1   5.2  3.84  1.36  5.04  3.68  2.32  1.92  6.48  3.68  ...  0.633   \n",
      "930  1  3.12  2.48  1.44  3.12  2.48  1.68  1.84  4.16  3.12  ...  0.651   \n",
      "931  0  3.12  2.48  1.44  3.12  2.48  1.68  1.84  4.16  3.12  ...  0.651   \n",
      "932  1   3.2  2.72  1.52  3.36  2.32   1.6  1.92  4.08   2.8  ...  0.692   \n",
      "933  1   3.2  2.72  1.52  3.36  2.32   1.6  1.92  4.08   2.8  ...  0.692   \n",
      "934  1  1.68   1.2  1.76   2.4  1.52   1.2  2.56   3.6  1.52  ...  0.559   \n",
      "935  0  1.68   1.2  1.76   2.4  1.52   1.2  2.56   3.6  1.52  ...  0.559   \n",
      "936  0  6.72  2.64  0.64  2.96  2.24  1.28     2  4.32  2.32  ...  0.689   \n",
      "937  1  6.72  2.64  0.64  2.96  2.24  1.28     2  4.32  2.32  ...  0.689   \n",
      "938  0  2.56  3.12  2.08  3.44  2.56  2.08  2.08  4.48  2.24  ...  0.733   \n",
      "939  1  2.56  3.12  2.08  3.44  2.56  2.08  2.08  4.48  2.24  ...  0.733   \n",
      "940  1   4.8  3.92  1.36     4  4.16  1.92   1.6  6.32  2.48  ...  0.688   \n",
      "941  1   4.8  3.92  1.36     4  4.16  1.92   1.6  6.32  2.48  ...  0.688   \n",
      "942  0  2.72  1.92  1.52   3.2  2.08  1.36  2.24  4.64  1.44  ...  0.592   \n",
      "943  1  2.72  1.92  1.52   3.2  2.08  1.36  2.24  4.64  1.44  ...  0.592   \n",
      "\n",
      "        43     44     45     46     47     48     49     50     51  \n",
      "0    0.695  0.726  0.669  0.543  0.745  0.637  0.615  0.548  0.621  \n",
      "1    0.692  0.594   0.72  0.648  0.745  0.637  0.615  0.548  0.621  \n",
      "2    0.692  0.594   0.72  0.648  0.745  0.637  0.615  0.548  0.621  \n",
      "3    0.675  0.785  0.674  0.583  0.745  0.637  0.615  0.548  0.621  \n",
      "4    0.675  0.785  0.674  0.583  0.745  0.637  0.615  0.548  0.621  \n",
      "5     0.75  0.703  0.676  0.547  0.745  0.637  0.615  0.548  0.621  \n",
      "6     0.75  0.703  0.676  0.547  0.745  0.637  0.615  0.548  0.621  \n",
      "7    0.749  0.654   0.72  0.631  0.745  0.637  0.615  0.548  0.621  \n",
      "8    0.749  0.654   0.72  0.631  0.745  0.637  0.615  0.548  0.621  \n",
      "9    0.626  0.718  0.664  0.634  0.745  0.637  0.615  0.548  0.621  \n",
      "10   0.626  0.718  0.664  0.634  0.745  0.637  0.615  0.548  0.621  \n",
      "11   0.722  0.722  0.636  0.478  0.745  0.637  0.615  0.548  0.621  \n",
      "12   0.722  0.722  0.636  0.478  0.745  0.637  0.615  0.548  0.621  \n",
      "13   0.669  0.718  0.751  0.575  0.745  0.637  0.615  0.548  0.621  \n",
      "14   0.669  0.718  0.751  0.575  0.745  0.637  0.615  0.548  0.621  \n",
      "15   0.673  0.693  0.703  0.584  0.745  0.637  0.615  0.548  0.621  \n",
      "16   0.673  0.693  0.703  0.584  0.745  0.637  0.615  0.548  0.621  \n",
      "17     0.7   0.71  0.747  0.638  0.687  0.698  0.656  0.673   0.61  \n",
      "18     0.7   0.71  0.747  0.638  0.687  0.698  0.656  0.673   0.61  \n",
      "19   0.671  0.652  0.739   0.59  0.687  0.698  0.656  0.673   0.61  \n",
      "20   0.671  0.652  0.739   0.59  0.687  0.698  0.656  0.673   0.61  \n",
      "21    0.75  0.644  0.702  0.472  0.687  0.698  0.656  0.673   0.61  \n",
      "22    0.75  0.644  0.702  0.472  0.687  0.698  0.656  0.673   0.61  \n",
      "23   0.698  0.736  0.726  0.623  0.687  0.698  0.656  0.673   0.61  \n",
      "24   0.698  0.736  0.726  0.623  0.687  0.698  0.656  0.673   0.61  \n",
      "25   0.774  0.661  0.636  0.566  0.687  0.698  0.656  0.673   0.61  \n",
      "26   0.774  0.661  0.636  0.566  0.687  0.698  0.656  0.673   0.61  \n",
      "27    0.73   0.72  0.699  0.609  0.687  0.698  0.656  0.673   0.61  \n",
      "28    0.73   0.72  0.699  0.609  0.687  0.698  0.656  0.673   0.61  \n",
      "29   0.647  0.643  0.715  0.572  0.687  0.698  0.656  0.673   0.61  \n",
      "..     ...    ...    ...    ...    ...    ...    ...    ...    ...  \n",
      "914  0.722  0.708  0.709  0.631  0.689  0.699  0.711  0.703  0.495  \n",
      "915  0.722  0.708  0.709  0.631  0.689  0.699  0.711  0.703  0.495  \n",
      "916  0.696  0.694  0.631   0.55  0.689  0.699  0.711  0.703  0.495  \n",
      "917  0.759  0.698  0.703  0.568  0.689  0.699  0.711  0.703  0.495  \n",
      "918  0.745   0.69  0.726   0.55  0.689  0.699  0.711  0.703  0.495  \n",
      "919  0.715  0.688    0.7  0.567  0.689  0.699  0.711  0.703  0.495  \n",
      "920  0.729  0.646  0.713  0.635  0.689  0.699  0.711  0.703  0.495  \n",
      "921  0.739  0.707  0.721  0.554  0.689  0.699  0.711  0.703  0.495  \n",
      "922  0.734  0.645  0.668  0.609  0.689  0.699  0.711  0.703  0.495  \n",
      "923  0.688  0.735  0.669  0.595  0.689  0.699  0.711  0.703  0.495  \n",
      "924  0.693  0.707  0.638  0.575  0.689  0.699  0.711  0.703  0.495  \n",
      "925  0.713  0.662  0.806  0.589  0.689  0.699  0.711  0.703  0.495  \n",
      "926  0.671   0.66  0.701  0.599  0.654  0.725   0.63  0.654  0.632  \n",
      "927  0.671   0.66  0.701  0.599  0.654  0.725   0.63  0.654  0.632  \n",
      "928   0.63  0.628  0.724  0.546  0.654  0.725   0.63  0.654  0.632  \n",
      "929   0.63  0.628  0.724  0.546  0.654  0.725   0.63  0.654  0.632  \n",
      "930  0.682  0.625  0.682  0.536  0.654  0.725   0.63  0.654  0.632  \n",
      "931  0.682  0.625  0.682  0.536  0.654  0.725   0.63  0.654  0.632  \n",
      "932  0.649  0.617  0.669  0.571  0.654  0.725   0.63  0.654  0.632  \n",
      "933  0.649  0.617  0.669  0.571  0.654  0.725   0.63  0.654  0.632  \n",
      "934  0.745  0.641  0.719  0.646  0.654  0.725   0.63  0.654  0.632  \n",
      "935  0.745  0.641  0.719  0.646  0.654  0.725   0.63  0.654  0.632  \n",
      "936  0.683  0.639  0.716  0.503  0.654  0.725   0.63  0.654  0.632  \n",
      "937  0.683  0.639  0.716  0.503  0.654  0.725   0.63  0.654  0.632  \n",
      "938  0.733   0.66  0.668  0.567  0.654  0.725   0.63  0.654  0.632  \n",
      "939  0.733   0.66  0.668  0.567  0.654  0.725   0.63  0.654  0.632  \n",
      "940  0.715  0.616  0.701  0.611  0.654  0.725   0.63  0.654  0.632  \n",
      "941  0.715  0.616  0.701  0.611  0.654  0.725   0.63  0.654  0.632  \n",
      "942  0.684  0.514  0.588  0.662  0.654  0.725   0.63  0.654  0.632  \n",
      "943  0.684  0.514  0.588  0.662  0.654  0.725   0.63  0.654  0.632  \n",
      "\n",
      "[944 rows x 51 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "load_data = pd.read_csv('DataAfterClean.csv')\n",
    "data_array = np.array(load_data)\n",
    "for line in data_array:\n",
    "    if line[0] == 'lose':\n",
    "        line[0] = 1\n",
    "    else:\n",
    "        line[0] = 0\n",
    "    if line[1] == 'LMS': \n",
    "        line[2:-10] = line[2:-10]*0.7\n",
    "    if line[1] == 'LCS' or line[1] == 'LEC':\n",
    "        line[2:-10] = line[2:-10]*0.8\n",
    "pd_data = pd.DataFrame(data_array)\n",
    "pd_data.drop(axis=1, columns=1, inplace=True)\n",
    "\n",
    "print(pd_data)\n",
    "pd_data.to_csv('dataAfterHash.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n",
      "42\n",
      "335\n",
      "334\n",
      "0.8860927152317881\n",
      "40\n",
      "41\n",
      "53\n",
      "55\n",
      "0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "data_array = np.array(pd_data)\n",
    "x = data_array[:,1:data_array.shape[1]]\n",
    "y = data_array[:,0:1]\n",
    "selection = [v for v in range(len(x)) if v%5 !=0] #traing_data\n",
    "selection2 = [v for v in range(len(x)) if v%5  == 0] #testing_data\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=1000,random_state=33)\n",
    "\n",
    "x2 = x[selection, :] \n",
    "x3 = x[selection2, :]\n",
    "y2 = y[selection, :]\n",
    "y3 = y[selection2, :]\n",
    "y_train = []\n",
    "y_test = []\n",
    "for line in y2:\n",
    "    y_train.append(line[0])\n",
    "for line in y3:\n",
    "    y_test.append(line[0])\n",
    "    \n",
    "clf = clf.fit(x2, y_train)\n",
    "\n",
    "y_predict_training = clf.predict(x2)\n",
    "y_predict_testing = clf.predict(x3)\n",
    "a1 = 0\n",
    "a2 = 0\n",
    "a3 = 0\n",
    "a4 = 0\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i] == 0 and y_predict_training[i] == 1:\n",
    "        a1 = a1 + 1\n",
    "    if y_train[i] == 1 and y_predict_training[i] == 0:\n",
    "        a2 = a2 + 1\n",
    "    if y_train[i] == 0 and y_predict_training[i] == 0:\n",
    "        a3 = a3 + 1\n",
    "    if y_train[i] == 1 and y_predict_training[i] == 1:\n",
    "        a4 = a4 + 1\n",
    "print(a1)\n",
    "print(a2)\n",
    "print(a3)\n",
    "print(a4)\n",
    "print((a3+a4)/len(y_train))\n",
    "\n",
    "\n",
    "a1 = 0\n",
    "a2 = 0\n",
    "a3 = 0\n",
    "a4 = 0\n",
    "\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i] == 0 and y_predict_testing[i] == 1:\n",
    "        a1 = a1 + 1\n",
    "    if y_test[i] == 1 and y_predict_testing[i] == 0:\n",
    "        a2 = a2 + 1\n",
    "    if y_test[i] == 0 and y_predict_testing[i] == 0:\n",
    "        a3 = a3 + 1\n",
    "    if y_test[i] == 1 and y_predict_testing[i] == 1:\n",
    "        a4 = a4 + 1\n",
    "\n",
    "print(a1)\n",
    "print(a2)\n",
    "print(a3)\n",
    "print(a4)\n",
    "\n",
    "print((a3+a4)/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.32 2.8800000000000003 1.2800000000000002 ... 0.615 0.548 0.621]\n",
      " [2.64 2.32 1.92 ... 0.615 0.548 0.621]\n",
      " [2.64 2.32 1.92 ... 0.615 0.548 0.621]\n",
      " ...\n",
      " [4.800000000000001 3.9200000000000004 1.36 ... 0.63 0.654 0.632]\n",
      " [2.72 1.92 1.52 ... 0.63 0.654 0.632]\n",
      " [2.72 1.92 1.52 ... 0.63 0.654 0.632]]\n",
      "[0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1]\n",
      "755\n",
      "104\n",
      "105\n",
      "275\n",
      "271\n",
      "0.7231788079470198\n",
      "31\n",
      "31\n",
      "62\n",
      "65\n",
      "0.671957671957672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\prospace\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "data_array = np.array(pd_data)\n",
    "\n",
    "x = data_array[:,1:data_array.shape[1]]\n",
    "y = data_array[:,0:1]\n",
    "selection = [v for v in range(len(x)) if v%5 !=0] #traing_data\n",
    "selection2 = [v for v in range(len(x)) if v%5 ==0] #testing_data\n",
    "print(x)\n",
    "lr = linear_model.LogisticRegression(penalty = 'l1')\n",
    "x2 = x[selection, :] \n",
    "x3 = x[selection2, :]\n",
    "y2 = y[selection, :]\n",
    "y3 = y[selection2, :]\n",
    "y_train = []\n",
    "y_test = []\n",
    "for line in y2:\n",
    "    y_train.append(line[0])\n",
    "for line in y3:\n",
    "    y_test.append(line[0])\n",
    "print(y_train)\n",
    "print(len(x2))\n",
    "lr.fit(x2,y_train)\n",
    "y_predict_training = lr.predict(x2)\n",
    "y_predict_testing = lr.predict(x3)\n",
    "a1 = 0\n",
    "a2 = 0\n",
    "a3 = 0\n",
    "a4 = 0\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i] == 0 and y_predict_training[i] == 1:\n",
    "        a1 = a1 + 1\n",
    "    if y_train[i] == 1 and y_predict_training[i] == 0:\n",
    "        a2 = a2 + 1\n",
    "    if y_train[i] == 0 and y_predict_training[i] == 0:\n",
    "        a3 = a3 + 1\n",
    "    if y_train[i] == 1 and y_predict_training[i] == 1:\n",
    "        a4 = a4 + 1\n",
    "print(a1)\n",
    "print(a2)\n",
    "print(a3)\n",
    "print(a4)\n",
    "print((a3+a4)/len(y_train))\n",
    "\n",
    "\n",
    "a1 = 0\n",
    "a2 = 0\n",
    "a3 = 0\n",
    "a4 = 0\n",
    "\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i] == 0 and y_predict_testing[i] == 1:\n",
    "        a1 = a1 + 1\n",
    "    if y_test[i] == 1 and y_predict_testing[i] == 0:\n",
    "        a2 = a2 + 1\n",
    "    if y_test[i] == 0 and y_predict_testing[i] == 0:\n",
    "        a3 = a3 + 1\n",
    "    if y_test[i] == 1 and y_predict_testing[i] == 1:\n",
    "        a4 = a4 + 1\n",
    "\n",
    "print(a1)\n",
    "print(a2)\n",
    "print(a3)\n",
    "print(a4)\n",
    "\n",
    "print((a3+a4)/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.32 2.8800000000000003 1.2800000000000002 ... 0.615 0.548 0.621]\n",
      " [2.64 2.32 1.92 ... 0.615 0.548 0.621]\n",
      " [2.64 2.32 1.92 ... 0.615 0.548 0.621]\n",
      " ...\n",
      " [4.800000000000001 3.9200000000000004 1.36 ... 0.63 0.654 0.632]\n",
      " [2.72 1.92 1.52 ... 0.63 0.654 0.632]\n",
      " [2.72 1.92 1.52 ... 0.63 0.654 0.632]]\n",
      "Train on 755 samples, validate on 189 samples\n",
      "Epoch 1/20\n",
      "755/755 [==============================] - 6s 7ms/step - loss: 0.6928 - acc: 0.4901 - val_loss: 0.6923 - val_acc: 0.4974\n",
      "Epoch 2/20\n",
      "755/755 [==============================] - 0s 350us/step - loss: 0.6904 - acc: 0.6079 - val_loss: 0.6892 - val_acc: 0.6190\n",
      "Epoch 3/20\n",
      "755/755 [==============================] - 0s 359us/step - loss: 0.6844 - acc: 0.7086 - val_loss: 0.6826 - val_acc: 0.6508\n",
      "Epoch 4/20\n",
      "755/755 [==============================] - 0s 351us/step - loss: 0.6730 - acc: 0.6967 - val_loss: 0.6752 - val_acc: 0.6085\n",
      "Epoch 5/20\n",
      "755/755 [==============================] - 0s 374us/step - loss: 0.6610 - acc: 0.6755 - val_loss: 0.6644 - val_acc: 0.6508\n",
      "Epoch 6/20\n",
      "755/755 [==============================] - 0s 376us/step - loss: 0.6554 - acc: 0.6358 - val_loss: 0.6591 - val_acc: 0.6508\n",
      "Epoch 7/20\n",
      "755/755 [==============================] - 0s 361us/step - loss: 0.6460 - acc: 0.6901 - val_loss: 0.6487 - val_acc: 0.6614\n",
      "Epoch 8/20\n",
      "755/755 [==============================] - 0s 373us/step - loss: 0.6408 - acc: 0.6993 - val_loss: 0.6433 - val_acc: 0.6614\n",
      "Epoch 9/20\n",
      "755/755 [==============================] - 0s 332us/step - loss: 0.6281 - acc: 0.6901 - val_loss: 0.6371 - val_acc: 0.6667\n",
      "Epoch 10/20\n",
      "755/755 [==============================] - 0s 342us/step - loss: 0.6246 - acc: 0.6993 - val_loss: 0.6428 - val_acc: 0.6349\n",
      "Epoch 11/20\n",
      "755/755 [==============================] - 0s 388us/step - loss: 0.6188 - acc: 0.6781 - val_loss: 0.6343 - val_acc: 0.6508\n",
      "Epoch 12/20\n",
      "755/755 [==============================] - 0s 367us/step - loss: 0.6113 - acc: 0.6834 - val_loss: 0.6322 - val_acc: 0.6561\n",
      "Epoch 13/20\n",
      "755/755 [==============================] - 0s 359us/step - loss: 0.6133 - acc: 0.6887 - val_loss: 0.6252 - val_acc: 0.6455\n",
      "Epoch 14/20\n",
      "755/755 [==============================] - 0s 391us/step - loss: 0.6085 - acc: 0.6927 - val_loss: 0.6312 - val_acc: 0.6561\n",
      "Epoch 15/20\n",
      "755/755 [==============================] - 0s 457us/step - loss: 0.6061 - acc: 0.6980 - val_loss: 0.6251 - val_acc: 0.6614\n",
      "Epoch 16/20\n",
      "755/755 [==============================] - 0s 365us/step - loss: 0.6009 - acc: 0.7007 - val_loss: 0.6247 - val_acc: 0.6614\n",
      "Epoch 17/20\n",
      "755/755 [==============================] - 0s 419us/step - loss: 0.6066 - acc: 0.6795 - val_loss: 0.6273 - val_acc: 0.6455\n",
      "Epoch 18/20\n",
      "755/755 [==============================] - 0s 399us/step - loss: 0.6019 - acc: 0.6848 - val_loss: 0.6235 - val_acc: 0.6614\n",
      "Epoch 19/20\n",
      "755/755 [==============================] - 0s 386us/step - loss: 0.5931 - acc: 0.7033 - val_loss: 0.6267 - val_acc: 0.6508\n",
      "Epoch 20/20\n",
      "755/755 [==============================] - 0s 374us/step - loss: 0.6038 - acc: 0.7046 - val_loss: 0.6230 - val_acc: 0.6614\n",
      "[[0.6689608  0.33103916]\n",
      " [0.30714175 0.6928582 ]\n",
      " [0.6915448  0.30845523]\n",
      " ...\n",
      " [0.62156725 0.3784328 ]\n",
      " [0.69947565 0.30052435]\n",
      " [0.66180384 0.33819616]]\n",
      "98\n",
      "102\n",
      "282\n",
      "273\n",
      "0.7350993377483444\n",
      "30\n",
      "34\n",
      "62\n",
      "63\n",
      "0.6613756613756614\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: UTF-8 -*-\n",
    "from sklearn.externals import joblib\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Embedding, Activation, merge, Input, Lambda, Reshape\n",
    "from keras.layers import Convolution1D, Flatten, Dropout, MaxPool1D, GlobalAveragePooling1D\n",
    "from keras.layers import LSTM, GRU, TimeDistributed, Bidirectional\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import initializers\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "x = data_array[:,1:data_array.shape[1]]\n",
    "print(x)\n",
    "y = []\n",
    "for line in data_array:\n",
    "    tmpline = []\n",
    "    if line[0] == 1:\n",
    "        tmpline.append(1);\n",
    "        tmpline.append(0);\n",
    "    else:\n",
    "        tmpline.append(0);\n",
    "        tmpline.append(1);\n",
    "    y.append(tmpline)\n",
    "\n",
    "# 划分训练/测试集\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# 将每个词用词典中的数值代替\n",
    "\n",
    "# One-hot\n",
    "\n",
    "model = Sequential()\n",
    "# 全连接层\n",
    "model.add(Dense(2048, input_shape=(50,), activation='softmax'))\n",
    "# DropOut层\n",
    "model.add(Dropout(0.4))\n",
    "# 全连接层+分类器\n",
    "#print(y_train)\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\t\t\t  \n",
    "y_train = np.array(y_train)\t\t\t  \n",
    "y_test = np.array(y_test)\t\n",
    "#print(y_train)\n",
    "#print(y_test)\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=20,\n",
    "          validation_data=(X_test, y_test))\n",
    "print(model.predict(X_train))\n",
    "\n",
    "\n",
    "\n",
    "y_predict_training = model.predict(X_train)\n",
    "y_predict_testing = model.predict(X_test)\n",
    "a1 = 0\n",
    "a2 = 0\n",
    "a3 = 0\n",
    "a4 = 0\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i][1] > 0.5 and y_predict_training[i][1] < 0.5:\n",
    "        a1 = a1 + 1\n",
    "    if y_train[i][1] < 0.5 and y_predict_training[i][1] > 0.5:\n",
    "        a2 = a2 + 1\n",
    "    if y_train[i][1] > 0.5 and y_predict_training[i][1] > 0.5:\n",
    "        a3 = a3 + 1\n",
    "    if y_train[i][1] < 0.5 and y_predict_training[i][1] < 0.5:\n",
    "        a4 = a4 + 1\n",
    "print(a1)\n",
    "print(a2)\n",
    "print(a3)\n",
    "print(a4)\n",
    "print((a3+a4)/len(y_train))\n",
    "\n",
    "\n",
    "a1 = 0\n",
    "a2 = 0\n",
    "a3 = 0\n",
    "a4 = 0\n",
    "\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i][1] > 0.5 and y_predict_testing[i][1] < 0.5:\n",
    "        a1 = a1 + 1\n",
    "    if y_test[i][1] < 0.5 and y_predict_testing[i][1] > 0.5:\n",
    "        a2 = a2 + 1\n",
    "    if y_test[i][1] > 0.5 and y_predict_testing[i][1] > 0.5:\n",
    "        a3 = a3 + 1\n",
    "    if y_test[i][1] < 0.5 and y_predict_testing[i][1] < 0.5:\n",
    "        a4 = a4 + 1\n",
    "\n",
    "print(a1)\n",
    "print(a2)\n",
    "print(a3)\n",
    "print(a4)\n",
    "print((a3+a4)/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_model_tree.m']"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model, 'train_model_mlp.m')\n",
    "joblib.dump(lr, 'train_model_linear.m')\n",
    "joblib.dump(clf, 'train_model_tree.m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G2    G2\n",
      "[4.4, 4.7, 2.5, 6.4, 4.3, 4.1, 2.8, 7.9, 4.4, 4.0, 2.6, 7.4, 3.6, 1.1, 3.3, 10.6, 2.9, 3.3, 3.2, 6.0, 3.25, 2.4, 0.85, 3.15, 2.3, 1.45, 1.2, 4.05, 2.3, 2.5, 1.2, 3.0, 2.15, 0.5, 1.5, 5.95, 2.15, 1.95, 1.1, 2.85, 0.642, 0.702, 0.665, 0.687, 0.541, 0.633, 0.63, 0.628, 0.724, 0.546]\n",
      "-1\n",
      "0\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "def predict_work(str1, str2):\n",
    "    all_team_data = pd.read_csv('teamDataAfterClean.csv')\n",
    "    team_data_array = np.array(all_team_data)\n",
    "    tmplineRate1 = []\n",
    "    tmplineRate2 = []\n",
    "    tmpdataline = []\n",
    "    dataline = []\n",
    "    for line in team_data_array:\n",
    "        if line[0] == str1:\n",
    "            tmpline2 = line[2:22]\n",
    "            if line[1] == 'LMS':\n",
    "                tmpline2 = tmpline2 * 0.7\n",
    "            if line[1] =='LCS'or line[1] == 'LEC':\n",
    "                tmpline2 = tmpline2 * 0.8\n",
    "            tmplineRate1 = line[22:27]\n",
    "            for x in tmpline2:\n",
    "                tmpdataline.append(x)\n",
    "\n",
    "    for line in team_data_array:       \n",
    "        if line[0] == str2:\n",
    "            print(line[0],'  ',str2)\n",
    "            tmpline2 = line[2:22]\n",
    "            if line[1] == 'LMS':\n",
    "                tmpline2 = tmpline2 * 0.7\n",
    "            if line[1] =='LCS'or line[1] == 'LEC':\n",
    "                tmpline2 = tmpline2 * 0.8\n",
    "            tmplineRate2 = line[22:27]\n",
    "            for x in tmpline2:\n",
    "                tmpdataline.append(x)\n",
    "                \n",
    "    #print(tmpdataline)\n",
    "    for x in tmplineRate1:\n",
    "        tmpdataline.append(x)\n",
    "    for x in tmplineRate2:\n",
    "        tmpdataline.append(x)\n",
    "        \n",
    "    print(tmpdataline)\n",
    "    \n",
    "    dataline.append(tmpdataline)\n",
    "    \n",
    "    final_x = np.array(dataline)\n",
    "    \n",
    "    model_mlp = joblib.load('train_model_mlp.m')\n",
    "    model_linear = joblib.load('train_model_linear.m')\n",
    "    model_tree = joblib.load('train_model_tree.m')\n",
    "    result1 = model_mlp.predict(final_x)\n",
    "    result2 = model_linear.predict(final_x)\n",
    "    result3 = model_tree.predict(final_x)\n",
    "    ans = 0\n",
    "    \n",
    "    if result1[0][1] > 0.5:\n",
    "        ans = ans + 1\n",
    "    else:\n",
    "        ans = ans - 1\n",
    "         \n",
    "    print(ans)\n",
    "    \n",
    "    if result2[0] == 1:\n",
    "        ans = ans + 1\n",
    "    else:\n",
    "        ans = ans - 1\n",
    "    \n",
    "    print(ans)\n",
    "    \n",
    "    if result3[0] == 1:\n",
    "        ans = ans + 1\n",
    "    else:\n",
    "        ans = ans - 1\n",
    "    \n",
    "    print(ans)\n",
    "    \n",
    "    if ans > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "print(predict_work('iG','G2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
